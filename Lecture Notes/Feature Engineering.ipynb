{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 12 Assignment Review\n",
    "\n",
    "Some examples of acceptable solutions to the Week 12 Assignment:\n",
    "\n",
    "- Jingmin: https://github.com/TTZUO/DAV-5400/blob/master/J_Chen_w12_assn.ipynb\n",
    "\n",
    "\n",
    "- Sara F: https://github.com/saramariaferrari/DAV-5400/blob/master/S_Ferrari_W12_assn.ipynb\n",
    "\n",
    "\n",
    "- Omar: https://github.com/OMS1996/DAV-5400/blob/master/Omar_Hussein_w12_assn%20FINAL.ipynb\n",
    "\n",
    "\n",
    "- Julian: https://github.com/gitjuli/DAV-5400/blob/master/J_Ruggiero_w12_assn.ipynb\n",
    "\n",
    "\n",
    "- Michael: https://github.com/vizelman/DAV-5400/blob/master/M_Vizelman_W12_assn.ipynb\n",
    "\n",
    "\n",
    "- Rachel: https://github.com/rachelward617/DAV-5400/blob/master/R_Ward_W12_Assn.ipynb\n",
    "\n",
    "\n",
    "- Eli: https://github.com/enwweiss/DAV-5400/blob/master/E_Weiss_w12_assn.ipynb\n",
    "\n",
    "\n",
    "- Zhihong: https://github.com/steinszzh/DAV-5400/blob/master/week12/zh_zhang_W12_assn.ipynb\n",
    "\n",
    "\n",
    "- Zhijing: https://github.com/zhijing-zhang/DAV-5400/blob/master/ZJ_Zhang_W12_assn.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 13: Modeling + Machine Learning with Python Libraries\n",
    "\n",
    "- A typical workflow for model development in Python relies on Pandas for data loading and cleaning followed by use of a modeling library to construct the actual model. \n",
    "\n",
    "\n",
    "- The specific Python library you choose to make use of is often dependent upon the type of model you are planning to construct: while many Python modeling libraries provide similar capabilities, it is up to you as an analytics practitioner to select the library that you believe is appropriate for the task at hand.\n",
    "\n",
    "\n",
    "- Prior to constructing a model, it is oftentimes beneficial to consider the addition of new attributes (or \"features\") to your data that you believe will either simplify or enhance the performance of your intended model. We create such \"features\" through the use of the __domain knowledge__ we have for the data we plan to make use of.\n",
    "\n",
    "\n",
    "- The addition of new attributes/features based on __domain knowledge__ is referred to as __Feature Engineering__.\n",
    "\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "What is a __\"feature\"__? In general, a __feature__ is an attribute or property shared by all of the independent variables we are planning to use for purposes of analysis or model building.\n",
    "\n",
    "In general, we can think of __feature engineering__ as creating new attributes for modeling based on both our __domain knowledge__ and the data we already have at hand.\n",
    "\n",
    "Some examples of __feature engineering__:\n",
    "\n",
    "- __Creation of indicator variables based on your domain knowledge__: For example, if we were working with historical data on US housing prices for the years 2000 - 2018, our domain knowledge could tell us that a major collapse in US housing prices occurred during the 2007 - 2012 timeframe due to the 2007 financial crisis, followed by a recovery that started during the year 2012. As such, if we believe that our overall housing price data content may have been tangibly impacted by the financial crisis, we could create a binary indicator variable that flags every data item within the 2007-2012 time period as potentially having been impacted by the financial crisis (assigning a '1' as the indicator), while all other data items are assigned a '0' for the indicator variable's value.\n",
    "\n",
    "\n",
    "- __Create \"Interaction\" Features__: Interaction features are new metrics we __derive__ from our existing data (NOTE: we've already touched on this concept this semester).Examples include creating new ratios / proportions from our existing data as well as binning / bucketing of numeric data and the de-structuring of date / time values.\n",
    "\n",
    "\n",
    "- __Combining Sparse Classifications__: With categorical data, \"sparse classes\" are those that have very few observations within a data set. Such \"sparse data\" can cause problems with a wide variety of predictive and machine learning models (e.g., causing the models to \"overfit\" the data use for model training purposes).  These challenges can oftentimes be overcome by combining sparse classes into new classifications that aggregate the sparse items based on some shared characteristic(s). How you identify such \"shared characteristics\" is highly subjective, i.e., you need to apply your __domain knowledge__ to identify them. \n",
    "\n",
    "\n",
    "- __Adding \"Dummy\" Variables__: As we've previously discussed, we cannot use raw categorical data as input to a model that requires numerical data. Instead, we create a new \"0/1\" binary indicator variable for each categorical data value.\n",
    "\n",
    "\n",
    "- __Removal of Unused Attributes/Features__: Oftentimes our data sets contain attributes that are of no use whatsoever for modeling purposes, such as unique ID's, text descriptions, and any other data that may be unlikely to be available if the model were placed into a \"production\" environment. Such attributes should be removed from the data you plan to use as input to a model.\n",
    "\n",
    "\n",
    "### How to Approach Feature Engineering?\n",
    "\n",
    "1. Using your __domain knowledge__, brainstorm ideas for potential new features relative to the data you have at hand.\n",
    "\n",
    "\n",
    "2. Create the features you believe will be useful (i.e., figure out how to add them to your data set).\n",
    "\n",
    "\n",
    "3. Test the features within your proposed analysis/model: do they improve the performance of the model? How relevant are they to the data set, i.e., do they provide new information that isn't available in any other attribute/feature already contained in the data set? Are they duplicative/redundant relative to other features/attributes contained within the data set? etc.\n",
    "\n",
    "\n",
    "4. Refine/improve your new features if needed.\n",
    "\n",
    "\n",
    "5. Go back to Step 1 (above) and consider whether the addition of other features would be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Modeling\n",
    "\n",
    "In general, the phrase __regression modeling__ refers to the process of estimating the strength of the relationship between one variable (the __\"Response\" (aka \"Dependent\") variable__) and one or more __\"Explanatory\" (aka \"Independent\") variables__. \n",
    "\n",
    "\n",
    "Regression modeling is widely used for purposes of __predicting the mean numerical value of the response variable__ when given specific values for the explanatory variables.\n",
    "\n",
    "\n",
    "There are many different types of regression models, and the characteristics of each vary depending on the type of response variable you are attempting to estimate. Some of the most widely used types of regression models include:\n",
    "\n",
    "- __Linear Regression__: Used for fitting a linear equation for a __continuous__ numeric response variable. The relationship between the response and explanatory variable(s) is assumed to be __linear__ in nature. The output is a __linear equation__. If there is only one explanatory variable, we use __Simple Linear Regression__. If there is more than one explanatory variable, we use __Multiple Linear Regression__.\n",
    "\n",
    "\n",
    "- __Polynomial Regression__: Used when a __non-linear__ relationship exists between a continuous response variable and explanatory variables that are either binary, discrete or continuous. The ouput is a __non-linear__ equation.\n",
    "\n",
    "\n",
    "- __Binary Logistic Regression__: The response variable is a __binary categorical variable__, while the explanatory variables can be either continuous, discrete or binary (e.g., dummy variables created from the values of a categorical variable).\n",
    "\n",
    "\n",
    "- __Multinomial Logistic Regression (MLR)__: The response variable is a categorical variable having __more than two (2) possible values__, while the explanatory variables can be either continuous, discrete or binary. There are many types of MLR models, and the exact type of MLR model you choose to use will be dependent on whether your response variable is __ordinal__ or __nominal__ in nature.\n",
    "\n",
    "\n",
    "- __\"Count\" Regression__: The response variable contains __non-negative__ (i.e., x >= 0) __discrete__ numeric \"count\" values while the explanatory variables can be either binary, discrete or continuous. Commonly used regression techniques for modeling count data include __Poisson Regression__ (where the response variable __must__ have a Poisson distribution), __Negative Binomial Regression__ (where the response variable need not have a Poisson distribution), and __Zero-Inflated Negative Binomial Regression__ (used when the response variable contains an excessive number of 'zero' values).\n",
    "\n",
    "\n",
    "\\*\\* Please note that there are __many__ other types of regression models (e.g., Lasso, Ridge, Partial Least Squares, etc.), and describing each of them is beyond the scope of this lecture.\\*\\*\n",
    "\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "Linear regression consists of finding the best fitting straight line through a set of data points. That line is called a __regression line__. An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate some random data to use for the plot\n",
    "x = np.random.randn(50) * 10\n",
    "\n",
    "y = np.random.randn(50) * 10\n",
    "\n",
    "# first define a Matplotlib figure\n",
    "plt.figure()\n",
    "\n",
    "# use Seaborn's regplot() function to plot the regression line for the \n",
    "sns.regplot(x, y)\n",
    "\n",
    "# give the plot a title using Matplotlib\n",
    "plt.title('Regression Plot');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple linear regression equations take the form of \n",
    "$y' = bx + a$ where $y'$ is the predicted value of the response variable for a given value of the explanatory variable $x$, $b$ is the slope of the regression line, $x$ is the value of the explanatory variable and $a$ is a constant representing the y-interecept of the regression line. \n",
    "\n",
    "\n",
    "The regression line is created by using the known values of the explanatory variable $x$ and the known values of the response variable $y$ to find the slope $b$ and y-intercept $a$ of the line that minimizes the sum of the squared errors of the predicted values of $y'$. \n",
    "\n",
    "\n",
    "We calculate values for the slope $b$ and the y-intercept $a$ using the means and standard deviations of the known values of $x$ and $y$ as well as the correlation coefficient $r$ of $x$ and $y$:\n",
    "\n",
    "$b = r * stddev(y)/stddev(x) $\n",
    "\n",
    "$a = mean(y) - b(mean(x))$\n",
    "\n",
    "__NOTE:__ The Python modeling libraries we are discussing below do these calculations for you. The calculations are shown here simply to provide further context.\n",
    "\n",
    "A simple linear regression example:\n",
    "\n",
    "http://onlinestatbook.com/2/regression/intro.html\n",
    "\n",
    "\n",
    "\n",
    "### Multiple Linear Regression\n",
    "\n",
    "When using multiple explanatory variables, the regression line __cannot be visualized within a two-dimensional space__. However, it is still relatively easy to compute: the regression line is estimated using the formula\n",
    "\n",
    "$y' = a + b1x1 + b2x2 + .. +bnxn$\n",
    "\n",
    "\n",
    "where $y'$ is the predicted value for the response variable for given values of the explanatory variables x1, x2, .., xn, and the $b$ values represent the __independent contributions__ of the corresponding explanatory variable to the prediction of the response variable.  \n",
    "\n",
    "For those who desire further explanation of the underlying mathematics:\n",
    "\n",
    "http://www.statsoft.com/Textbook/Multiple-Regression\n",
    "\n",
    "http://faculty.cas.usf.edu/mbrannick/regression/Reg2IV.html\n",
    "\n",
    "\n",
    "### Binary Logistic Regression\n",
    "\n",
    "In binary logistic regression, the response variable is __binary__, i.e., it contains data encoded as either '1' (e.g., 'True') or '0' (e.g., 'False'). In other words, logistic regression is used for __classification__ problems.\n",
    "\n",
    "Logistic regression finds the best fitting __mathematical model__ (not a simple linear equation) to describe the relationship between the binary response variable and the explanatory variable(s). The values it generates are the coefficients of a formula to predict a __logit transformation__ (aka \"__log odds\"__) which is the logarithm of the odds $p$ of the propability of the presence of the characteristic of interest. \n",
    "\n",
    "$logit(p) = ln(p/(1-p)) = b0 + b1x1 + b2x2 + .. + bnxn$\n",
    "\n",
    "where $p$ = the probability of the __presence__ of a characteristic,\n",
    "\n",
    "$(1-p)$ = the probability of the __abscence__ of a characteristic,\n",
    "\n",
    "$b0$ = is a constant value\n",
    "\n",
    "$b1, b2, ..,bn$ = the regression coefficients for the explanatory variables $x1, x2, .., xn$\n",
    "\n",
    "Further explanation is available here:\n",
    "\n",
    "https://www.statisticssolutions.com/what-is-logistic-regression/\n",
    "\n",
    "\n",
    "While a wide variety of statistical modeling libraries are available for use within __Python__, we will limit our focus to those mentioned in the PfDA text: __Patsy__, __statsmodels__, and __scikit-learn__. (__NOTE__: All three of these libraries are usually provided automatically when you install __Anaconda__.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patsy: A Python Library for Describing Statistical Models\n",
    "\n",
    "- Most often used for describing linear models.\n",
    "\n",
    "\n",
    "- Uses a simple syntactical structure.\n",
    "\n",
    "\n",
    "- Developed to mimic the way in which the R and S programming languages allow a user to specify linear models + models that have a linear component.\n",
    "\n",
    "\n",
    "- Basic approach is: 1) create a formula string, e.g., 'y ~ x0 + x1' based on the contents of a data set; 2) use patsy.dmatrices() to generate the design matrix for a linear model. The resulting \"dmatrix\" is then fed into a pre-built statistical modeling function from a library such as __statsmodels__.\n",
    "\n",
    "\n",
    "- Some examples are provided in the PfDA text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statsmodels: A Python Library for Fitting Many Types of Statistical Models\n",
    "\n",
    "__statsmodels__ contains tools for performing statistical tests, data exploration and visualization, and supports many types of statistical models including:\n",
    "\n",
    "- Linear models, generalized linear models, logistic regression, robust linear models, etc.\n",
    "\n",
    "- ANOVA (Analysis of Variance) models\n",
    "\n",
    "- Time series modeling\n",
    "\n",
    "\n",
    "Let's take a look at __statsmodels'__ linear modeling tools.\n",
    "\n",
    "\n",
    "### Linear Models with statsmodels\n",
    "\n",
    "- __statsmodels__ supports many kinds of linear regression models, including ordinary least squares.\n",
    "\n",
    "\n",
    "- Linear models are specified in __statsmodels__ either via arrays or formulas.\n",
    "\n",
    "\n",
    "__An example__: A multiple linear model specified via arrays using some random data. First, let's implement the model using linear algebra/matrix mathematics (i.e., without using __statsmodels__):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy, pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12946849, -1.21275292,  0.50422488],\n",
       "       [ 0.30291036, -0.43574176, -0.25417986],\n",
       "       [-0.32852189, -0.02530153,  0.13835097],\n",
       "       [-0.35147471, -0.71960511, -0.25821463],\n",
       "       [ 1.2432688 , -0.37379916, -0.52262905]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dnorm is a function used to generate normally distributed data having\n",
    "# a particular mean and variance\n",
    "def dnorm(mean, variance, size=1):\n",
    "    # check to ensure size is an integer\n",
    "    if isinstance(size, int):\n",
    "        size = size,\n",
    "    # now generate an array of size 'size' where each item is sampled from\n",
    "    # a normal distribution, multiplied by the provided variance and then\n",
    "    # has the provided mean value added to it\n",
    "    return mean + np.sqrt(variance) * np.random.randn(*size)\n",
    "\n",
    "# For reproducibility, we set the random generator seed to a fixed value\n",
    "np.random.seed(12345)\n",
    "\n",
    "# define the number of items we want to have in our data set\n",
    "N = 100\n",
    "\n",
    "# np.c_ is a 'concatenate' method, so we are concatenating 3 1 dimensional\n",
    "# arrays of size 'N'. So we have 3 explanatory variables in our X array\n",
    "X = np.c_[dnorm(0, 0.4, size=N),\n",
    "          dnorm(0, 0.6, size=N),\n",
    "          dnorm(0, 0.2, size=N)]\n",
    "\n",
    "# let's take a look at the first 5 rows of X\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps is an array of random error terms for our linear equation: must be same\n",
    "# size as the arrays we created above\n",
    "eps = dnorm(0, 0.1, size=N)\n",
    "\n",
    "# beta is our set of coefficients for the three explanatory variables\n",
    "beta = [0.1, 0.3, 0.5]\n",
    "\n",
    "# now define our linear equation by adding our random error 'eps' to\n",
    "# the dot product of array 'X' and array 'Beta'; the results in variable \n",
    "# 'y' are our estimated values/output of our linear equation\n",
    "y = np.dot(X, beta) + eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now what is 'X'? It is an array containing 100 rows ('N' == 100) of \n",
    "# 3 columns of random data\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is 'y'? It is an array containing the results of our \n",
    "# linear estimation equation: np.dot(X, beta) + eps\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.88126723 -0.02808985 -0.36896015]]\n",
      " \n",
      "0.07423798034615169\n"
     ]
    }
   ],
   "source": [
    "# check the corresponding elements of arrays 'X' and 'y'\n",
    "print(X[[5]])\n",
    "print(' ')\n",
    "\n",
    "# array y contains the predicted values\n",
    "print(y[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07423798])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now apply the linear equation we defined above to X[[5]] to see if the result\n",
    "# matches the content of y[5]\n",
    "np.dot(X[[5]], beta) + eps[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we are modeling $y' = (b1x1 + \\epsilon) + (b2x2 + \\epsilon).. + (bnxn + \\epsilon)$ where the $b$ values represent the __independent contributions__ of the corresponding explanatory variable to the prediction of the response variable, and $\\epsilon$ is the random error we add to each term. __What's missing__?\n",
    "\n",
    "__We are missing an intercept value__. The general form of a multiple linear model is $y' = a + b1x1 + b2x2 + .. +bnxn$ where $a$ is the intercept value. So we would need to further refine our example by adding a value for the intercept. \n",
    "\n",
    "Furthermore, __we usually do not know the coefficients for the explanatory variables beforehand__ (the contents of the 'beta' array in this example).\n",
    "\n",
    "Now let's look at an example of how __statsmodels'__ ordinary least squares ('OLS()') function can estimate the regression coefficients for us. We need to start by adding a constant 'intercept' value to our array 'X':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.12946849, -1.21275292,  0.50422488],\n",
       "       [ 1.        ,  0.30291036, -0.43574176, -0.25417986],\n",
       "       [ 1.        , -0.32852189, -0.02530153,  0.13835097],\n",
       "       [ 1.        , -0.35147471, -0.71960511, -0.25821463],\n",
       "       [ 1.        ,  1.2432688 , -0.37379916, -0.52262905]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add an intercept value to the array 'X' we've defined\n",
    "X_model = sm.add_constant(X)\n",
    "X_model[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17826108, 0.22303962, 0.50095093])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now instead of using np.dot, use statsmodels' OLS() function to fit\n",
    "# an ordinary least squares regression equation to our random data\n",
    "model = sm.OLS(y, X)\n",
    "\n",
    "# now get the results of the regression\n",
    "results = model.fit()\n",
    "\n",
    "# check the regression coefficients for our 3 explanatory variables (X1, X2, X3)\n",
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.430\n",
      "Model:                            OLS   Adj. R-squared:                  0.413\n",
      "Method:                 Least Squares   F-statistic:                     24.42\n",
      "Date:                Fri, 22 Mar 2019   Prob (F-statistic):           7.44e-12\n",
      "Time:                        17:15:35   Log-Likelihood:                -34.305\n",
      "No. Observations:                 100   AIC:                             74.61\n",
      "Df Residuals:                      97   BIC:                             82.42\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.1783      0.053      3.364      0.001       0.073       0.283\n",
      "x2             0.2230      0.046      4.818      0.000       0.131       0.315\n",
      "x3             0.5010      0.080      6.237      0.000       0.342       0.660\n",
      "==============================================================================\n",
      "Omnibus:                        4.662   Durbin-Watson:                   2.201\n",
      "Prob(Omnibus):                  0.097   Jarque-Bera (JB):                4.098\n",
      "Skew:                           0.481   Prob(JB):                        0.129\n",
      "Kurtosis:                       3.243   Cond. No.                         1.74\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# print the detailed diagnostic data for the model\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intepreting the Model Statistics from Statsmodels' OLS() Function\n",
    "- $R^2$: Often referred to as \"Goodness of fit\"; measures how much of the variation in the response variable $y'$ is explained by variation in the explanatory variable(s). In general, __the larger the value of $R^2$, the more accurate the model is__. However, relatively large values of $R^2$ can also be an indication of the model being \"overfit\" to the training data.\n",
    "\n",
    "\n",
    "- __Adjusted__ $R^2$: Increases if a new variable added to the model improves the fit of the model by more than would be expected by sheer chance. When comparing two models derived from the same data, models with __higher__ __Adjusted__ $R^2$ scores are preferable to those having relatively lower scores. \n",
    "\n",
    "\n",
    "- __AIC__: Akaike Information Criteria is a model selection metric that estimates the relative quality of a statistical model for a given set of data. When comparing two models derived from the same data, models with __lower__ __AIC__ scores are preferable to those having relatively higher scores.\n",
    "\n",
    "\n",
    "- __BIC__: Bayesian Information Criteria is another model selection metric that estimates the unexplained variation in the response variable relative to the given explanatory variables. __BIC__ also imposes a \"complexity\" penalty when the number of explanatory variables used is increased.  When comparing two models derived from the same data, models with __lower__ __BIC__ scores are preferable to those having relatively higher scores.\n",
    "\n",
    "\n",
    "- __F Statistic__: Indicates whether a significant amount of variance in the response variable $y'$ is explained by the model. When comparing two models derived from the same data, models with __higher__ __F Statistic__ scores are preferable to those having relatively lower scores. \n",
    "\n",
    "\n",
    "- __Log Likelihood__: A measure of how well a model fits the underlying data. When comparing two models derived from the same data, models with __higher__ __Log Likelihood__ scores are preferable to those having relatively lower scores. \n",
    "\n",
    "\n",
    "- __p values__: Measure the statistical significance of the explanatory variables in your model. While you are free to select the significance level on your own, most often 0.05 is used as the maximum bound for significance. As such, if any variable in your model is shown to have a __p value__ that exceeds 0.05, consider removing it from the model to see whether the fit/model selection metrics improve.\n",
    "\n",
    "\n",
    "#### What if our data is instead contained within a Pandas DataFrame? \n",
    "\n",
    "Note that the above example has assumed our data is contained within a numpy array. What if the data is instead housed within a Pandas DataFrame?\n",
    "\n",
    "One crucial difference is that __we do not need to add a constant value for the intercept if our data is contained within a DataFrame__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col0</th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.129468</td>\n",
       "      <td>-1.212753</td>\n",
       "      <td>0.504225</td>\n",
       "      <td>0.427863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.302910</td>\n",
       "      <td>-0.435742</td>\n",
       "      <td>-0.254180</td>\n",
       "      <td>-0.673480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.328522</td>\n",
       "      <td>-0.025302</td>\n",
       "      <td>0.138351</td>\n",
       "      <td>-0.090878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.351475</td>\n",
       "      <td>-0.719605</td>\n",
       "      <td>-0.258215</td>\n",
       "      <td>-0.489494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.243269</td>\n",
       "      <td>-0.373799</td>\n",
       "      <td>-0.522629</td>\n",
       "      <td>-0.128941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       col0      col1      col2         y\n",
       "0 -0.129468 -1.212753  0.504225  0.427863\n",
       "1  0.302910 -0.435742 -0.254180 -0.673480\n",
       "2 -0.328522 -0.025302  0.138351 -0.090878\n",
       "3 -0.351475 -0.719605 -0.258215 -0.489494\n",
       "4  1.243269 -0.373799 -0.522629 -0.128941"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a data frame containing our 3 explanatory variables\n",
    "data = pd.DataFrame(X, columns=['col0', 'col1', 'col2'])\n",
    "\n",
    "# add our response variable to the data frame\n",
    "data['y'] = y\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept    0.033559\n",
       "col0         0.176149\n",
       "col1         0.224826\n",
       "col2         0.514808\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the statsmodel/Patsy 'formula' api to specify the linear model:\n",
    "results = smf.ols('y ~ col0 + col1 + col2', data=data).fit()\n",
    "results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.435\n",
      "Model:                            OLS   Adj. R-squared:                  0.418\n",
      "Method:                 Least Squares   F-statistic:                     24.68\n",
      "Date:                Fri, 22 Mar 2019   Prob (F-statistic):           6.37e-12\n",
      "Time:                        17:33:43   Log-Likelihood:                -33.835\n",
      "No. Observations:                 100   AIC:                             75.67\n",
      "Df Residuals:                      96   BIC:                             86.09\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.0336      0.035      0.952      0.343      -0.036       0.104\n",
      "col0           0.1761      0.053      3.320      0.001       0.071       0.281\n",
      "col1           0.2248      0.046      4.851      0.000       0.133       0.317\n",
      "col2           0.5148      0.082      6.304      0.000       0.353       0.677\n",
      "==============================================================================\n",
      "Omnibus:                        4.504   Durbin-Watson:                   2.223\n",
      "Prob(Omnibus):                  0.105   Jarque-Bera (JB):                3.957\n",
      "Skew:                           0.475   Prob(JB):                        0.138\n",
      "Kurtosis:                       3.222   Cond. No.                         2.38\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# check the t-value of \n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study: Linear Regression with statsmodels\n",
    "\n",
    "https://github.com/justmarkham/DAT4/blob/master/notebooks/08_linear_regression.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn: A General-Purpose Machine Learning Toolkit\n",
    "\n",
    "__scikit-learn__ contains tools for machine learning tasks such as classification, clustering and prediction as well as for modeling tasks such as model selection & evaluation, data transformation and data loading \n",
    "\n",
    "Like __statsmodels__, __scikit-learn__ is widely used and is considered to be a reliable and robust modeling library.\n",
    "\n",
    "\n",
    "### Logistic Regression example with scikit-learn\n",
    "\n",
    "From the PfDA text: Use the 'Titanic' data set which contains passenger survival rate data for the ocean-going vessel of that name that sank in 1912. \n",
    "\n",
    "Our goal is to __try to predict whether or not a passenger would have been more likely than not to survive the sinking of the Titanic__ based on the data contained in the data set (i.e., build a __predictive model__).\n",
    "\n",
    "- First, we will fit a logistic regression model on a __training__ data set using the data set's __PClass__, __Age__, and __Sex__ attributes.\n",
    "\n",
    "\n",
    "- Then we evaluate the model using a __testing__ data set that is exclusive of the data contained in the __training__ data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the LogisticRegression() function from sklearn's 'linear_model' sub-library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# load the training + testing data sets for the Titanic data\n",
    "train = pd.read_csv('https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/datasets/titanic/train.csv')\n",
    "test = pd.read_csv('https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/datasets/titanic/test.csv')\n",
    "\n",
    "# display the first four rows of the data set\n",
    "train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>IsFemale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                              Name     Sex   Age  SibSp  \\\n",
       "0          892       3                  Kelly, Mr. James    male  34.5      0   \n",
       "1          893       3  Wilkes, Mrs. James (Ellen Needs)  female  47.0      1   \n",
       "2          894       2         Myles, Mr. Thomas Francis    male  62.0      0   \n",
       "3          895       3                  Wirz, Mr. Albert    male  27.0      0   \n",
       "\n",
       "   Parch  Ticket    Fare Cabin Embarked  IsFemale  \n",
       "0      0  330911  7.8292   NaN        Q         0  \n",
       "1      0  363272  7.0000   NaN        S         1  \n",
       "2      0  240276  9.6875   NaN        Q         0  \n",
       "3      0  315154  8.6625   NaN        S         0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check the first four rows of the test data\n",
    "# Note the lack of a \"Survived\" indicator !!!\n",
    "test[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many records in the training data set?\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many people survived in the training set?\n",
    "train.Survived.values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3838383838383838"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what percentage of the training set survived?\n",
    "train.Survived.values.sum() / train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__: Since we know that 38.3% of the people in the training set survived, we could achieve a training model accuracy of (1 - .383) = 61.7% by simply predicting \"Did not survive\" for each passenger. This metric is referred to as the __null error rate__.  When evaluating the performance of a binary logistic regression model, always check to see whether the accuracy you are attaining exceeds the __null error rate__. If not, your model is unlikely to be of any value.\n",
    "\n",
    "\n",
    "Missing data will generally prevent the fitting of a model via either __scikit-learn__ or __statsmodels__, so we must first check both the training and testing data for missing data values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the training data for null values\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the training data, the __Age__, __Cabin__ and __Embarked__ attributes all have missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age             86\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             1\n",
       "Cabin          327\n",
       "Embarked         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the test data for null values\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the testing data, the __Age__, __Fare__ and __Cabin__  attributes all have missing data.\n",
    "\n",
    "So if we want to use __Age__ as part of our predictive model, we need to somehow fill the missing __Age__ values. In this instance, the author of the PfDA text chooses to use the relatively simple process of filling the missing values with the __median__ Age value found within the __training__ data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the median 'Age' value within the training data set\n",
    "impute_value = train['Age'].median()\n",
    "\n",
    "# now fill the missing 'Age' values in both the training and testing data sets\n",
    "train['Age'] = train['Age'].fillna(impute_value)\n",
    "test['Age'] = test['Age'].fillna(impute_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, __we create a dummy indicator for the 'Sex' categorical variable__: the new dummy variable 'IsFemale' contains a '1' if the 'Sex' value for a passenger is 'Female' and a '0' otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dummy variable for the 'Sex' attribute in both the training and\n",
    "# testing data sets\n",
    "train['IsFemale'] = (train['Sex'] == 'female').astype(int)\n",
    "test['IsFemale'] = (test['Sex'] == 'female').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define our predictive model using the attributes we want to use as explanatory variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.,  0., 22.],\n",
       "       [ 1.,  1., 38.],\n",
       "       [ 3.,  1., 26.],\n",
       "       [ 1.,  1., 35.],\n",
       "       [ 3.,  0., 35.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a vector containing the names of the attributes to use\n",
    "predictors = ['Pclass', 'IsFemale', 'Age']\n",
    "\n",
    "# create a subset of the training data using ONLY the selected explanatory variables\n",
    "X_train = train[predictors].values\n",
    "\n",
    "# create a subset of the testing data using ONLY the selected explanatory variables\n",
    "X_test = test[predictors].values\n",
    "\n",
    "# isolate the response indicator for the training data\n",
    "y_train = train['Survived'].values\n",
    "\n",
    "# sanity check on training data\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check on response indicator\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\James T\\Anaconda3\\envs\\DAV5400\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're using the LogisticRegression() method for this model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# fit the model: X_train contains our explanatory variables while \n",
    "# y_train contains the response variable\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7946127946127947"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the accuracy of the model relative to the training data set\n",
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from above we calculated the __null error rate__ for our data to be 61.7%. The model we've generated has an accuracy score of 79.46%. As such, our model appears to be useful.\n",
    "\n",
    "__What can we learn by examining the regression model coefficients for the explanatory variables?__\n",
    "\n",
    "While the actual numeric values of the coefficients of a regression model are often very difficult to interpret, we can use the fact that a coefficient is either positive or negative to determine what effect of an __increase__ in the value of a given variable will have on the predicted value of the response variable. \n",
    "\n",
    "In this binary logistic regression example, the value of the response variable is the \"log odds\" of the binary outcome being either a '0' or '1' ('0' meaning the passenger was more likely to perish while '1' indicates the passenger was more likely to survive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pclass', 'IsFemale', 'Age']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.07373582,  2.52093733, -0.02864864]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the model coefficients for the explanatory variables\n",
    "print(predictors)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we see that:\n",
    "\n",
    "- __Passenger Class__: An increase in the value of 'Passenger_Class' is associated with a __decreased__ likelihood of survival, i.e., passengers in lower classes of service were more likely to perish than were passengers in relatively higher classes of service.\n",
    "\n",
    "\n",
    "- __IsFemale__: Being female increased the likelihood of survival\n",
    "\n",
    "\n",
    "- __Age__: The older the passenger, the less likely they were to survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 1, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate predictions for the test data using our new model\n",
    "y_predict = model.predict(X_test)\n",
    "y_predict[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'y_predict' array contains predictions that answer the question: \"__Was a given passenger more likely than not to have survived the sinking of the Titanic?__\"\n",
    "\n",
    "If we had the actual __survived/perished__ indicators for the test data, we could now check the performance of the model via a variety of error metrics (e.g., accuracy, specificity, precision, recall, AUC, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study: Logistic Regression with scikit-learn\n",
    "\n",
    "https://nbviewer.jupyter.org/gist/justmarkham/6d5c061ca5aee67c4316471f8c2ae976"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
